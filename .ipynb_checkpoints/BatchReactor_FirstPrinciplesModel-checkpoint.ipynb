{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sEo_u79qqdF"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.integrate import odeint\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gISNEmbqte_"
   },
   "outputs": [],
   "source": [
    "Ad      = 4.4e16\n",
    "Ed      = 140.06e3\n",
    "Ap      = 1.7e11/60\n",
    "Ep      = 16.9e3/0.239\n",
    "deltaHp = -82.2e3\n",
    "UA      = 33.3083 #%18.8445;\n",
    "Qc      = 650\n",
    "Qs      = 12.41e-2\n",
    "V       = 0.5\n",
    "Tc      = 27\n",
    "Tamb    = 27\n",
    "Cpc     = 4.184\n",
    "R       = 8.3145\n",
    "alpha   = 1.212827\n",
    "beta    = 0.000267\n",
    "epsilon = 0.5\n",
    "theta   = 1.25\n",
    "m1      = 450\n",
    "cp1     = 4.184\n",
    "mjCpj   = (18*4.184)+(240*0.49)\n",
    "cp2     = 187\n",
    "cp3     = 110.58 #%J/molK\n",
    "cp4     = 84.95\n",
    "m5      = 220\n",
    "cp5     = 0.49\n",
    "m6      = 7900\n",
    "cp6     = 0.49\n",
    "M0      = 0.7034\n",
    "I0      = 4.5e-3\n",
    "\n",
    "# Define Batch Reactor model\n",
    "def br(x,t,u,Ad):\n",
    "    # Inputs:\n",
    "    # Coolant flow rate\n",
    "    F = u*16.667\n",
    "\n",
    "    # States (4):\n",
    "    # Initiator\n",
    "    Ii  = x[0]\n",
    "    # Monomer\n",
    "    M  = x[1]\n",
    "    # Reactor temperature\n",
    "    Tr = x[2]\n",
    "    # Jacket temperature\n",
    "    Tj = x[3]\n",
    "\n",
    "    Ri    = Ad*Ii*(np.exp(-Ed/(R*(Tr+273.15))))\n",
    "    Rp    = Ap*(Ii**epsilon)*(M**(theta)*(np.exp(-Ep/(R*(Tr+273.15)))))\n",
    "    mrCpr = m1*cp1+ Ii*cp2*V + M*cp3*V +(M0-M)*cp4*V+ m5*cp5 + m6*cp6\n",
    "    Qpr   = alpha*(Tr-Tc)**beta\n",
    "\n",
    "    # Computing the rate of change of I, M, Tr, Tj using Differential Equations\n",
    "    dy1_dt = -Ri\n",
    "    dy2_dt = -Rp\n",
    "    dy3_dt = (Rp*V*(-deltaHp)-UA*(Tr-Tj)+Qc+Qs-Qpr)/mrCpr\n",
    "    dy4_dt = (UA*(Tr-Tj)-F*Cpc*(Tj-Tc))/mjCpj\n",
    "\n",
    "    # Return xdot:\n",
    "    xdot = np.zeros(4)\n",
    "    xdot[0] = dy1_dt\n",
    "    xdot[1] = dy2_dt\n",
    "    xdot[2] = dy3_dt\n",
    "    xdot[3] = dy4_dt\n",
    "\n",
    "    return xdot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6F1XdO4eq37O"
   },
   "outputs": [],
   "source": [
    "class BR3 (gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.action_space = spaces.Box(low=0.25, high=0.75, shape=(1,), dtype=np.float32)\n",
    "\n",
    "        self.observation_space = spaces.Box(low=0, high=100, shape=(2,), dtype=np.float32)\n",
    "\n",
    "        self.t = np.linspace(0,7200,7201)\n",
    "        self.i= 0\n",
    "\n",
    "        Tr_ref = pd.read_csv('Trajectory2.csv') # Enter the CSV file directory for setpoints\n",
    "        self.a1 = Tr_ref.values.tolist()\n",
    "        self.sp = self.a1[self.i][0] # Saving the setpoints in an array\n",
    "\n",
    "       # Assign Initial conditions to the variables\n",
    "        self.I = 4.5e-3     # Initiator\n",
    "        self.M = 0.7034     # Monomer\n",
    "        self.Tr = 45.0      # Reactor Temperature\n",
    "        self.Tj = 40.0      # Jacket Temperature\n",
    "\n",
    "        self.state = self.Tr ,self.sp\n",
    "\n",
    "        # Creating an NumPy array to store the values of I, M, Tr and Tj for give it as an argument to the br_equation function\n",
    "\n",
    "        self.y0= np.empty(4)\n",
    "        self.y0[0] = self.I        # Initiator Concentration\n",
    "        self.y0[1] = self.M        # Monomer Concentration\n",
    "        self.y0[2] = self.Tr       # Reactor Temperature\n",
    "        self.y0[3] = self.Tj       # Jacket Temperature\n",
    "\n",
    "\n",
    "        self.time_step= 7200 # Number of computation steps\n",
    "\n",
    "    # Step function to give the coolant flow rate (in LPM) to the model, returning the next state and computed reward\n",
    "    def step(self, action): # Note that the action is an array\n",
    "\n",
    "        action = action[0]\n",
    "        u = action\n",
    "\n",
    "        ts = [self.t[self.i],self.t[self.i+1]]\n",
    "\n",
    "        y = scipy.integrate.odeint(br,self.y0, ts , args=(u,4.4e16),)\n",
    "\n",
    "        x = np.round(y, decimals=4)\n",
    "\n",
    "        self.I = x[-1][0]\n",
    "        self.M = x[-1][1]\n",
    "        self.Tr = x[-1][2]\n",
    "        self.Tj = x[-1][3]\n",
    "\n",
    "        self.y0= np.empty(4)\n",
    "        self.y0[0] = self.I        # Initiator Concentration\n",
    "        self.y0[1] = self.M        # Monomer Concentration\n",
    "        self.y0[2] = self.Tr       # Reactor Temperature\n",
    "        self.y0[3] = self.Tj       # Jacket Temperature\n",
    "\n",
    "\n",
    "        # Data saving snippet when using the model - Saves the data to a csv file\n",
    "        data = [self.sp, self.Tr, self.Tj, action]\n",
    "        with open('data.csv', 'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(data)\n",
    "\n",
    "        self.sp=self.a1[self.i][0]\n",
    "        self.i += 1\n",
    "\n",
    "        # Calculate the difference between Setpoint and Reactor Temperature (Process Variable)\n",
    "        difference = self.sp - self.Tr\n",
    "        self.reward = 0\n",
    "\n",
    "        # Calculate the modulus of the difference\n",
    "        error = abs(difference)\n",
    "\n",
    "        # These rewards can be modified in accordance with the requirements\n",
    "        if error <= 0.5:\n",
    "            self.reward = +100\n",
    "        elif error <= 1:\n",
    "            self.reward = +50\n",
    "        elif error <= 3:\n",
    "            self.reward = +25\n",
    "        elif error <= 4:\n",
    "            self.reward = +10\n",
    "        else:\n",
    "            self.reward = -100\n",
    "\n",
    "        if self.i>= self.time_step:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        self.state = self.Tr , self.sp\n",
    "\n",
    "        return self.state, self.reward ,done ,info\n",
    "\n",
    "    def reset(self):\n",
    "        self.I  = 4.5e-3\n",
    "        self.M  = 0.7034\n",
    "        self.Tr = 45.0\n",
    "        self.Tj = 40.0\n",
    "        self.i  = 0\n",
    "\n",
    "        self.sp = self.a1[self.i][0]\n",
    "        self.state=self.Tr,self.sp\n",
    "\n",
    "        self.y0= np.empty(4)\n",
    "        self.y0[0] = self.I        # Initiator Concentration\n",
    "        self.y0[1] = self.M        # Monomer Concentration\n",
    "        self.y0[2] = self.Tr       # Reactor Temperature\n",
    "        self.y0[3] = self.Tj       # Jacket Temperature\n",
    "\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.integrate\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# If you're running on Google Colab, uncomment these lines to install stable-baselines3\n",
    "# !pip install stable-baselines3\n",
    "# !pip install tensorboard\n",
    "\n",
    "# Create a mock Trajectory2.csv file with setpoints for training\n",
    "# (Replace this with loading your actual file if available)\n",
    "def create_mock_trajectory():\n",
    "    # Create a simple temperature profile that varies from 45 to 70 degrees\n",
    "    time_steps = 7201\n",
    "    setpoints = np.zeros((time_steps, 1))\n",
    "    \n",
    "    # Initial phase - maintain at 45°C\n",
    "    setpoints[:1000] = 45.0\n",
    "    \n",
    "    # Ramp up to 70°C\n",
    "    ramp_indices = np.arange(1000, 3000)\n",
    "    setpoints[1000:3000] = 45.0 + (70.0 - 45.0) * (ramp_indices - 1000) / 2000\n",
    "    \n",
    "    # Hold at 70°C\n",
    "    setpoints[3000:5000] = 70.0\n",
    "    \n",
    "    # Ramp down to 50°C\n",
    "    ramp_indices = np.arange(5000, 6000)\n",
    "    setpoints[5000:6000] = 70.0 - (70.0 - 50.0) * (ramp_indices - 5000) / 1000\n",
    "    \n",
    "    # Hold at 50°C until the end\n",
    "    setpoints[6000:] = 50.0\n",
    "    \n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(setpoints)\n",
    "    df.to_csv('Trajectory2.csv', index=False, header=False)\n",
    "    \n",
    "    return setpoints\n",
    "\n",
    "# Check if Trajectory2.csv exists, if not create it\n",
    "if not os.path.exists('Trajectory2.csv'):\n",
    "    create_mock_trajectory()\n",
    "\n",
    "# Reactor parameters\n",
    "Ad      = 4.4e16\n",
    "Ed      = 140.06e3\n",
    "Ap      = 1.7e11/60\n",
    "Ep      = 16.9e3/0.239\n",
    "deltaHp = -82.2e3\n",
    "UA      = 33.3083 #%18.8445;\n",
    "Qc      = 650\n",
    "Qs      = 12.41e-2\n",
    "V       = 0.5\n",
    "Tc      = 27\n",
    "Tamb    = 27\n",
    "Cpc     = 4.184\n",
    "R       = 8.3145\n",
    "alpha   = 1.212827\n",
    "beta    = 0.000267\n",
    "epsilon = 0.5\n",
    "theta   = 1.25\n",
    "m1      = 450\n",
    "cp1     = 4.184\n",
    "mjCpj   = (18*4.184)+(240*0.49)\n",
    "cp2     = 187\n",
    "cp3     = 110.58 #%J/molK\n",
    "cp4     = 84.95\n",
    "m5      = 220\n",
    "cp5     = 0.49\n",
    "m6      = 7900\n",
    "cp6     = 0.49\n",
    "M0      = 0.7034\n",
    "I0      = 4.5e-3\n",
    "\n",
    "# Define Batch Reactor model\n",
    "def br(x, t, u, Ad):\n",
    "    # Inputs:\n",
    "    # Coolant flow rate\n",
    "    F = u*16.667\n",
    "    # States (4):\n",
    "    # Initiator\n",
    "    Ii  = x[0]\n",
    "    # Monomer\n",
    "    M  = x[1]\n",
    "    # Reactor temperature\n",
    "    Tr = x[2]\n",
    "    # Jacket temperature\n",
    "    Tj = x[3]\n",
    "    Ri    = Ad*Ii*(np.exp(-Ed/(R*(Tr+273.15))))\n",
    "    Rp    = Ap*(Ii**epsilon)*(M**(theta)*(np.exp(-Ep/(R*(Tr+273.15)))))\n",
    "    mrCpr = m1*cp1+ Ii*cp2*V + M*cp3*V +(M0-M)*cp4*V+ m5*cp5 + m6*cp6\n",
    "    Qpr   = alpha*(Tr-Tc)**beta\n",
    "    # Computing the rate of change of I, M, Tr, Tj using Differential Equations\n",
    "    dy1_dt = -Ri\n",
    "    dy2_dt = -Rp\n",
    "    dy3_dt = (Rp*V*(-deltaHp)-UA*(Tr-Tj)+Qc+Qs-Qpr)/mrCpr\n",
    "    dy4_dt = (UA*(Tr-Tj)-F*Cpc*(Tj-Tc))/mjCpj\n",
    "    # Return xdot:\n",
    "    xdot = np.zeros(4)\n",
    "    xdot[0] = dy1_dt\n",
    "    xdot[1] = dy2_dt\n",
    "    xdot[2] = dy3_dt\n",
    "    xdot[3] = dy4_dt\n",
    "    return xdot\n",
    "\n",
    "class BR3(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = spaces.Box(low=0.25, high=0.75, shape=(1,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=0, high=100, shape=(2,), dtype=np.float32)\n",
    "        self.t = np.linspace(0, 7200, 7201)\n",
    "        self.i = 0\n",
    "        Tr_ref = pd.read_csv('Trajectory2.csv') # Enter the CSV file directory for setpoints\n",
    "        self.a1 = Tr_ref.values.tolist()\n",
    "        self.sp = self.a1[self.i][0] # Saving the setpoints in an array\n",
    "        # Assign Initial conditions to the variables\n",
    "        self.I = 4.5e-3     # Initiator\n",
    "        self.M = 0.7034     # Monomer\n",
    "        self.Tr = 45.0      # Reactor Temperature\n",
    "        self.Tj = 40.0      # Jacket Temperature\n",
    "        self.state = self.Tr, self.sp\n",
    "        # Creating an NumPy array to store the values of I, M, Tr and Tj for give it as an argument to the br_equation function\n",
    "        self.y0 = np.empty(4)\n",
    "        self.y0[0] = self.I        # Initiator Concentration\n",
    "        self.y0[1] = self.M        # Monomer Concentration\n",
    "        self.y0[2] = self.Tr       # Reactor Temperature\n",
    "        self.y0[3] = self.Tj       # Jacket Temperature\n",
    "        self.time_step = 7200 # Number of computation steps\n",
    "        \n",
    "        # For logging data\n",
    "        if os.path.exists('data.csv'):\n",
    "            os.remove('data.csv')\n",
    "        with open('data.csv', 'w') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Setpoint', 'Tr', 'Tj', 'Action'])\n",
    "    \n",
    "    # Step function to give the coolant flow rate (in LPM) to the model, returning the next state and computed reward\n",
    "    def step(self, action): # Note that the action is an array\n",
    "        action = action[0]\n",
    "        u = action\n",
    "        ts = [self.t[self.i], self.t[self.i+1]]\n",
    "        y = scipy.integrate.odeint(br, self.y0, ts, args=(u, 4.4e16))\n",
    "        x = np.round(y, decimals=4)\n",
    "        self.I = x[-1][0]\n",
    "        self.M = x[-1][1]\n",
    "        self.Tr = x[-1][2]\n",
    "        self.Tj = x[-1][3]\n",
    "        self.y0 = np.empty(4)\n",
    "        self.y0[0] = self.I        # Initiator Concentration\n",
    "        self.y0[1] = self.M        # Monomer Concentration\n",
    "        self.y0[2] = self.Tr       # Reactor Temperature\n",
    "        self.y0[3] = self.Tj       # Jacket Temperature\n",
    "        # Data saving snippet when using the model - Saves the data to a csv file\n",
    "        data = [self.sp, self.Tr, self.Tj, action]\n",
    "        with open('data.csv', 'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(data)\n",
    "        \n",
    "        self.i += 1\n",
    "        if self.i < len(self.a1):\n",
    "            self.sp = self.a1[self.i][0]\n",
    "        \n",
    "        # Calculate the difference between Setpoint and Reactor Temperature (Process Variable)\n",
    "        difference = self.sp - self.Tr\n",
    "        self.reward = 0\n",
    "        # Calculate the modulus of the difference\n",
    "        error = abs(difference)\n",
    "        # These rewards can be modified in accordance with the requirements\n",
    "        if error <= 0.5:\n",
    "            self.reward = +100\n",
    "        elif error <= 1:\n",
    "            self.reward = +50\n",
    "        elif error <= 3:\n",
    "            self.reward = +25\n",
    "        elif error <= 4:\n",
    "            self.reward = +10\n",
    "        else:\n",
    "            self.reward = -100\n",
    "        \n",
    "        if self.i >= self.time_step:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        info = {}\n",
    "        self.state = self.Tr, self.sp\n",
    "        return np.array(self.state, dtype=np.float32), self.reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.I  = 4.5e-3\n",
    "        self.M  = 0.7034\n",
    "        self.Tr = 45.0\n",
    "        self.Tj = 40.0\n",
    "        self.i  = 0\n",
    "        self.sp = self.a1[self.i][0]\n",
    "        self.state = self.Tr, self.sp\n",
    "        self.y0 = np.empty(4)\n",
    "        self.y0[0] = self.I        # Initiator Concentration\n",
    "        self.y0[1] = self.M        # Monomer Concentration\n",
    "        self.y0[2] = self.Tr       # Reactor Temperature\n",
    "        self.y0[3] = self.Tj       # Jacket Temperature\n",
    "        \n",
    "        # For logging data - reset the data file\n",
    "        if os.path.exists('data.csv'):\n",
    "            os.remove('data.csv')\n",
    "        with open('data.csv', 'w') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Setpoint', 'Tr', 'Tj', 'Action'])\n",
    "            \n",
    "        return np.array(self.state, dtype=np.float32)\n",
    "\n",
    "# Define a CNN+LSTM neural network architecture for the RL agent\n",
    "class CNNLSTMNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, sequence_length=10):\n",
    "        super(CNNLSTMNetwork, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # CNN layers for feature extraction\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        \n",
    "        # LSTM layer for sequential decision making\n",
    "        self.lstm = nn.LSTM(input_size=32, hidden_size=hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Fully connected layers for action prediction\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_dim)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Reshape for CNN (batch_size, input_dim, sequence_length)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Apply CNN layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        # Reshape for LSTM (batch_size, sequence_length, features)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Apply LSTM layer\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Take the output from the last time step\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Apply fully connected layers\n",
    "        x = F.relu(self.fc1(lstm_out))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Define Deep Q-Network Agent with Experience Replay\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, hidden_size=64, sequence_length=10):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0   # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        # Create Q networks\n",
    "        self.model = CNNLSTMNetwork(state_size, hidden_size, action_size, sequence_length)\n",
    "        self.target_model = CNNLSTMNetwork(state_size, hidden_size, action_size, sequence_length)\n",
    "        self.update_target_model()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        # Store past states for sequence input\n",
    "        self.state_memory = deque(maxlen=sequence_length)\n",
    "        \n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def get_sequence(self, state):\n",
    "        # Add current state to memory\n",
    "        self.state_memory.append(state)\n",
    "        \n",
    "        # If we don't have enough states, pad with zeros\n",
    "        if len(self.state_memory) < self.sequence_length:\n",
    "            padding = [np.zeros_like(state) for _ in range(self.sequence_length - len(self.state_memory))]\n",
    "            sequence = padding + list(self.state_memory)\n",
    "        else:\n",
    "            sequence = list(self.state_memory)\n",
    "            \n",
    "        return np.array(sequence)\n",
    "    \n",
    "    def act(self, state):\n",
    "        # Get sequence of states\n",
    "        state_sequence = self.get_sequence(state)\n",
    "        \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # Explore: choose random action\n",
    "            return np.random.uniform(0.25, 0.75, (1,))\n",
    "        \n",
    "        # Exploit: choose best action from Q values\n",
    "        state_sequence = torch.FloatTensor(state_sequence).unsqueeze(0)  # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state_sequence)\n",
    "            \n",
    "        # Convert q_values to continuous action (between 0.25 and 0.75)\n",
    "        action_value = q_values.numpy()[0][0]\n",
    "        # Scale the output to the action range\n",
    "        scaled_action = 0.25 + (0.75 - 0.25) * (action_value + 1) / 2\n",
    "        scaled_action = np.clip(scaled_action, 0.25, 0.75)\n",
    "        \n",
    "        return np.array([scaled_action])\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "            \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # Get sequence for current and next state\n",
    "            state_sequence = self.get_sequence(state)\n",
    "            next_state_sequence = self.get_sequence(next_state)\n",
    "            \n",
    "            state_sequence = torch.FloatTensor(state_sequence).unsqueeze(0)\n",
    "            next_state_sequence = torch.FloatTensor(next_state_sequence).unsqueeze(0)\n",
    "            \n",
    "            # Calculate target Q value\n",
    "            target = reward\n",
    "            if not done:\n",
    "                with torch.no_grad():\n",
    "                    target = reward + self.gamma * torch.max(self.target_model(next_state_sequence))\n",
    "            \n",
    "            # Get current Q values\n",
    "            current_q = self.model(state_sequence)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = self.criterion(current_q, torch.FloatTensor([[target]]))\n",
    "            \n",
    "            # Perform optimization\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Function to run a test episode with trained agent\n",
    "def run_test_episode(env, agent):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Initialize arrays to store data for plotting\n",
    "    setpoints = []\n",
    "    temperatures = []\n",
    "    jacket_temps = []\n",
    "    actions = []\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        # Store data for plotting\n",
    "        setpoints.append(state[1])\n",
    "        temperatures.append(state[0])\n",
    "        jacket_temps.append(env.Tj)\n",
    "        actions.append(action[0])\n",
    "    \n",
    "    return setpoints, temperatures, jacket_temps, actions, total_reward\n",
    "\n",
    "# Train the DQN agent\n",
    "def train_dqn_agent(episodes=100, batch_size=64):\n",
    "    env = BR3()\n",
    "    state_size = 2\n",
    "    action_size = 1\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "                \n",
    "        # Update target model periodically\n",
    "        if e % 10 == 0:\n",
    "            agent.update_target_model()\n",
    "            \n",
    "        scores.append(total_reward)\n",
    "        print(f\"Episode: {e+1}/{episodes}, Score: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n",
    "    \n",
    "    # Plot training performance\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(scores)\n",
    "    plt.title('Training Performance')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.savefig('training_performance.png')\n",
    "    \n",
    "    return env, agent\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "    \n",
    "    # Train the agent\n",
    "    print(\"Training DQN agent with CNN+LSTM architecture...\")\n",
    "    env, agent = train_dqn_agent(episodes=100)\n",
    "    \n",
    "    # Run a test episode and plot results\n",
    "    print(\"Running test episode...\")\n",
    "    setpoints, temperatures, jacket_temps, actions, total_reward = run_test_episode(env, agent)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot temperature vs setpoint\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(setpoints, 'r-', label='Setpoint')\n",
    "    plt.plot(temperatures, 'b-', label='Reactor Temperature')\n",
    "    plt.title('Reactor Temperature Control with RL')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Temperature (°C)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot jacket temperature\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(jacket_temps, 'g-', label='Jacket Temperature')\n",
    "    plt.title('Jacket Temperature')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Temperature (°C)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot control actions\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(actions, 'k-', label='Coolant Flow Rate')\n",
    "    plt.title('Control Actions (Normalized Coolant Flow Rate)')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Flow Rate (normalized)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('reinforcement_learning_results.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Test episode completed with total reward: {total_reward}\")\n",
    "    print(\"Results saved as 'reinforcement_learning_results.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1c58a4-c4c0-41fd-aa81-238204abb88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RL Controller implementation...\n",
      "Got reference trajectory with shape: (7201, 1)\n",
      "Training RL controller on cpu...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import random\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "# Fix random state for consistency\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Simplified network architecture - removed CNN for speed\n",
    "class FastDQNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=64):\n",
    "        super(FastDQNetwork, self).__init__()\n",
    "        \n",
    "        # Simpler LSTM architecture\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1,  # Single feature input\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,  # Reduced from 2 to 1\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Simpler Q-value prediction layers\n",
    "        self.q_layers = nn.Sequential(\n",
    "            nn.Linear(hidden_size, action_size)  # Direct mapping\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        # Handle different input shapes efficiently\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.FloatTensor(state)\n",
    "            \n",
    "        batch_size = state.size(0) if state.dim() > 1 else 1\n",
    "        seq_len = state.size(-1) if state.dim() > 1 else state.size(0)\n",
    "        \n",
    "        # Reshape to [batch_size, seq_len, features]\n",
    "        if state.dim() == 1:\n",
    "            # Single sample: [seq_len] -> [1, seq_len, 1]\n",
    "            x = state.view(1, seq_len, 1)\n",
    "        elif state.dim() == 2:\n",
    "            # Batch: [batch_size, seq_len] -> [batch_size, seq_len, 1]\n",
    "            x = state.view(batch_size, seq_len, 1)\n",
    "        else:\n",
    "            # Already in correct shape\n",
    "            x = state\n",
    "            \n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Get Q-values for each action\n",
    "        q_values = self.q_layers(last_out)\n",
    "        \n",
    "        return q_values\n",
    "\n",
    "# Optimized Replay Buffer with numpy arrays instead of deque operations\n",
    "class FastReplayBuffer:\n",
    "    def __init__(self, capacity=10000, state_size=50):\n",
    "        self.capacity = capacity\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Pre-allocate memory for all experiences\n",
    "        self.states = np.zeros((capacity, state_size), dtype=np.float32)\n",
    "        self.next_states = np.zeros((capacity, state_size), dtype=np.float32)\n",
    "        self.actions = np.zeros(capacity, dtype=np.int64)\n",
    "        self.rewards = np.zeros(capacity, dtype=np.float32)\n",
    "        self.dones = np.zeros(capacity, dtype=np.float32)\n",
    "        \n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        # Store experience\n",
    "        self.states[self.position] = state\n",
    "        self.actions[self.position] = action\n",
    "        self.rewards[self.position] = reward\n",
    "        self.next_states[self.position] = next_state\n",
    "        self.dones[self.position] = float(done)\n",
    "        \n",
    "        # Update position and size\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        # Sample random indices\n",
    "        indices = np.random.choice(self.size, batch_size, replace=False)\n",
    "        \n",
    "        # Return experiences as tensors\n",
    "        return (\n",
    "            torch.FloatTensor(self.states[indices]),\n",
    "            torch.LongTensor(self.actions[indices]),\n",
    "            torch.FloatTensor(self.rewards[indices]),\n",
    "            torch.FloatTensor(self.next_states[indices]),\n",
    "            torch.FloatTensor(self.dones[indices])\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "# NEW: System model for simulation\n",
    "class SystemDynamicsModel:\n",
    "    def __init__(self, inertia=1.0, damping=0.1, dt=0.1):\n",
    "        \"\"\"Simple dynamic system model (e.g., motor or physical system)\n",
    "        \n",
    "        Args:\n",
    "            inertia: System inertia coefficient\n",
    "            damping: System damping coefficient\n",
    "            dt: Time step for discrete simulation\n",
    "        \"\"\"\n",
    "        self.inertia = inertia\n",
    "        self.damping = damping\n",
    "        self.dt = dt\n",
    "        \n",
    "        # System state\n",
    "        self.position = 0.0\n",
    "        self.velocity = 0.0\n",
    "        \n",
    "    def reset(self, position=0.0, velocity=0.0):\n",
    "        \"\"\"Reset system to initial state\"\"\"\n",
    "        self.position = position\n",
    "        self.velocity = velocity\n",
    "        return self.position\n",
    "        \n",
    "    def step(self, control_force):\n",
    "        \"\"\"Apply control force and update system state\n",
    "        \n",
    "        Simple second-order dynamics model: m*a + b*v = F\n",
    "        Where:\n",
    "            m = inertia\n",
    "            b = damping coefficient\n",
    "            F = control force\n",
    "        \"\"\"\n",
    "        # Calculate acceleration from control force and current state\n",
    "        acceleration = (control_force - self.damping * self.velocity) / self.inertia\n",
    "        \n",
    "        # Update velocity and position using simple Euler integration\n",
    "        self.velocity += acceleration * self.dt\n",
    "        self.position += self.velocity * self.dt\n",
    "        \n",
    "        return self.position\n",
    "\n",
    "# NEW: Sensor interface for real-world applications\n",
    "class SensorInterface:\n",
    "    def __init__(self, use_simulation=True, sensor_noise=0.01):\n",
    "        \"\"\"Sensor interface that can read from simulation or real hardware\n",
    "        \n",
    "        Args:\n",
    "            use_simulation: If True, use simulated data. If False, read from real sensors.\n",
    "            sensor_noise: Amount of noise to add to simulated sensor readings\n",
    "        \"\"\"\n",
    "        self.use_simulation = use_simulation\n",
    "        self.sensor_noise = sensor_noise\n",
    "        \n",
    "        # If not using simulation, set up hardware communication\n",
    "        if not use_simulation:\n",
    "            try:\n",
    "                # This would be replaced with actual hardware setup code\n",
    "                # e.g., import serial, GPIO setup, etc.\n",
    "                print(\"Setting up hardware communication...\")\n",
    "                # self.serial_port = serial.Serial('/dev/ttyUSB0', 9600)\n",
    "            except Exception as e:\n",
    "                print(f\"Hardware setup failed: {e}\")\n",
    "                print(\"Falling back to simulation mode\")\n",
    "                self.use_simulation = True\n",
    "    \n",
    "    def read_position(self, simulated_position=None):\n",
    "        \"\"\"Read current position from sensor or simulation\"\"\"\n",
    "        if self.use_simulation:\n",
    "            # Add noise to simulated position\n",
    "            return simulated_position + np.random.normal(0, self.sensor_noise)\n",
    "        else:\n",
    "            # This would be replaced with actual sensor reading code\n",
    "            # e.g., self.serial_port.write(b'READ_POS\\n')\n",
    "            # return float(self.serial_port.readline())\n",
    "            return 0.0  # Placeholder\n",
    "\n",
    "# NEW: Actuator interface for real-world applications\n",
    "class ActuatorInterface:\n",
    "    def __init__(self, use_simulation=True, control_min=-1.0, control_max=1.0):\n",
    "        \"\"\"Actuator interface that can send commands to simulation or real hardware\n",
    "        \n",
    "        Args:\n",
    "            use_simulation: If True, return values for simulation. If False, send to real actuators.\n",
    "            control_min: Minimum control signal value\n",
    "            control_max: Maximum control signal value\n",
    "        \"\"\"\n",
    "        self.use_simulation = use_simulation\n",
    "        self.control_min = control_min\n",
    "        self.control_max = control_max\n",
    "        \n",
    "        # If not using simulation, set up hardware communication\n",
    "        if not use_simulation:\n",
    "            try:\n",
    "                # This would be replaced with actual hardware setup code\n",
    "                print(\"Setting up actuator communication...\")\n",
    "                # self.serial_port = serial.Serial('/dev/ttyUSB1', 9600)\n",
    "            except Exception as e:\n",
    "                print(f\"Actuator setup failed: {e}\")\n",
    "                print(\"Falling back to simulation mode\")\n",
    "                self.use_simulation = True\n",
    "    \n",
    "    def send_control(self, control_signal):\n",
    "        \"\"\"Send control signal to actuator or return for simulation\"\"\"\n",
    "        # Clip control signal to valid range\n",
    "        control_signal = np.clip(control_signal, self.control_min, self.control_max)\n",
    "        \n",
    "        if self.use_simulation:\n",
    "            return control_signal\n",
    "        else:\n",
    "            # This would be replaced with actual actuator control code\n",
    "            # e.g., self.serial_port.write(f'SET_PWM {control_signal}\\n')\n",
    "            print(f\"Sending control signal: {control_signal}\")\n",
    "            return control_signal\n",
    "\n",
    "# Modified: RL Controller Environment (previously TrajectoryEnvironment)\n",
    "class RLControlEnvironment:\n",
    "    def __init__(self, reference_trajectory, window_size=50, control_discretization=20,\n",
    "                 use_simulation=True, control_effort_weight=0.1):\n",
    "        \"\"\"Environment for the RL Controller\n",
    "        \n",
    "        Args:\n",
    "            reference_trajectory: Target trajectory to follow\n",
    "            window_size: Size of state window (recent positions and setpoints)\n",
    "            control_discretization: Number of discrete control actions\n",
    "            use_simulation: If True, use simulated system. If False, use real hardware.\n",
    "            control_effort_weight: Weight of control effort term in reward (λ)\n",
    "        \"\"\"\n",
    "        self.reference_trajectory = reference_trajectory\n",
    "        self.window_size = window_size\n",
    "        self.use_simulation = use_simulation\n",
    "        self.control_effort_weight = control_effort_weight\n",
    "        \n",
    "        # Create discrete action space (control signals)\n",
    "        self.control_range = [-1.0, 1.0]  # Normalized control range\n",
    "        self.action_values = np.linspace(\n",
    "            self.control_range[0], \n",
    "            self.control_range[1], \n",
    "            control_discretization\n",
    "        )\n",
    "        self.action_size = len(self.action_values)\n",
    "        \n",
    "        # State is the window of previous values and setpoints\n",
    "        # [positions, setpoints] concatenated\n",
    "        self.state_size = window_size\n",
    "        \n",
    "        # Initialize system model, sensors, and actuators\n",
    "        self.system = SystemDynamicsModel()\n",
    "        self.sensor = SensorInterface(use_simulation=use_simulation)\n",
    "        self.actuator = ActuatorInterface(use_simulation=use_simulation)\n",
    "        \n",
    "        # Current position in the reference trajectory\n",
    "        self.current_idx = window_size\n",
    "        self.max_idx = len(reference_trajectory) - 1\n",
    "        \n",
    "        # History of positions and controls\n",
    "        self.position_history = np.zeros(window_size)\n",
    "        self.control_history = np.zeros(window_size)\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to starting point\"\"\"\n",
    "        self.current_idx = self.window_size\n",
    "        self.system.reset()\n",
    "        \n",
    "        # Reset history\n",
    "        self.position_history = np.zeros(self.window_size)\n",
    "        self.control_history = np.zeros(self.window_size)\n",
    "        \n",
    "        # Initial state combines position history and setpoint trajectory\n",
    "        # For simplicity, we'll just use the position history as the state\n",
    "        state = self.position_history.reshape(-1)\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take control action and return new state, reward, done\"\"\"\n",
    "        # Get the control signal corresponding to this action\n",
    "        control_signal = self.action_values[action]\n",
    "        \n",
    "        # Send control signal to actuator\n",
    "        applied_control = self.actuator.send_control(control_signal)\n",
    "        \n",
    "        # Apply control to system (simulation or real)\n",
    "        if self.use_simulation:\n",
    "            # Update system with control input\n",
    "            new_position = self.system.step(applied_control)\n",
    "            \n",
    "            # Read position from sensor (with noise)\n",
    "            measured_position = self.sensor.read_position(new_position)\n",
    "        else:\n",
    "            # In real-world mode, just read the sensor after applying control\n",
    "            # The control signal has already been sent by the actuator interface\n",
    "            measured_position = self.sensor.read_position()\n",
    "        \n",
    "        # Get current setpoint from reference trajectory\n",
    "        current_setpoint = self.reference_trajectory[self.current_idx][0]\n",
    "        \n",
    "        # Calculate tracking error\n",
    "        tracking_error = abs(measured_position - current_setpoint)\n",
    "        \n",
    "        # Calculate control effort penalty\n",
    "        control_effort = abs(control_signal)\n",
    "        \n",
    "        # Calculate reward: -(tracking error) - λ(control effort)\n",
    "        reward = -tracking_error - self.control_effort_weight * control_effort\n",
    "        \n",
    "        # Update histories\n",
    "        self.position_history = np.append(self.position_history[1:], measured_position)\n",
    "        self.control_history = np.append(self.control_history[1:], control_signal)\n",
    "        \n",
    "        # Move to next setpoint\n",
    "        self.current_idx += 1\n",
    "        done = self.current_idx >= self.max_idx\n",
    "        \n",
    "        # Get new state\n",
    "        if not done:\n",
    "            # State is the history of positions\n",
    "            next_state = self.position_history.reshape(-1)\n",
    "        else:\n",
    "            next_state = np.zeros(self.window_size)  # Default state when done\n",
    "            \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def get_current_setpoint(self):\n",
    "        \"\"\"Get the current target setpoint\"\"\"\n",
    "        if self.current_idx < len(self.reference_trajectory):\n",
    "            return self.reference_trajectory[self.current_idx][0]\n",
    "        return None\n",
    "\n",
    "# Optimized DQN Agent - Minimal changes for control application\n",
    "class FastDQNAgent:\n",
    "    def __init__(self, state_size, action_size, hidden_size=64, device='cpu'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.device = device\n",
    "        \n",
    "        # Hyperparameters - increased learning rate and batch size for faster convergence\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.97  # Faster decay\n",
    "        self.learning_rate = 0.003  # Increased from 0.001\n",
    "        self.batch_size = 128  # Increased from 64\n",
    "        self.update_target_every = 5  # More frequent updates\n",
    "        \n",
    "        # Networks\n",
    "        self.q_network = FastDQNetwork(state_size, action_size, hidden_size).to(device)\n",
    "        self.target_network = FastDQNetwork(state_size, action_size, hidden_size).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()  # target network in eval mode\n",
    "        \n",
    "        # Optimizer with higher learning rate\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        # Memory\n",
    "        self.memory = FastReplayBuffer(capacity=10000, state_size=state_size)\n",
    "        \n",
    "        # Tracking\n",
    "        self.episode_count = 0\n",
    "        \n",
    "        # Create evaluation mode flag to avoid unnecessary computation\n",
    "        self.training_mode = True\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"Select action using epsilon-greedy policy\"\"\"\n",
    "        self.training_mode = training\n",
    "        \n",
    "        if training and random.random() < self.epsilon:\n",
    "            # Exploration: random action\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # Exploitation: best action from Q-network\n",
    "        with torch.no_grad():  # Ensure we don't compute gradients\n",
    "            # Convert to tensor if it's a numpy array\n",
    "            if isinstance(state, np.ndarray):\n",
    "                state = torch.FloatTensor(state)\n",
    "            \n",
    "            # Add batch dimension if needed\n",
    "            if state.dim() == 1:\n",
    "                state = state.unsqueeze(0)  # [L] -> [1, L]\n",
    "                \n",
    "            # Add channel dimension if needed\n",
    "            if state.dim() == 2 and state.size(1) != 1:\n",
    "                state = state.unsqueeze(2)  # [B, L] -> [B, L, 1]\n",
    "                \n",
    "            state = state.to(self.device)\n",
    "            q_values = self.q_network(state)\n",
    "            return torch.argmax(q_values).item()\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train on a batch from replay memory\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0  # Not enough samples\n",
    "        \n",
    "        # Sample batch\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # Move to device\n",
    "        states = states.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        \n",
    "        # Get current Q values - more efficient indexing\n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Get next Q values - more efficient with no_grad\n",
    "        with torch.no_grad():\n",
    "            # Double DQN\n",
    "            next_actions = torch.argmax(self.q_network(next_states), dim=1)\n",
    "            next_q = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            # Apply terminal state masking and calculate targets\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "        \n",
    "        # Calculate loss and optimize\n",
    "        loss = nn.MSELoss()(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update target network weights with soft update instead of hard copy\"\"\"\n",
    "        tau = 0.1  # Soft update parameter\n",
    "        for target_param, param in zip(self.target_network.parameters(), self.q_network.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate\"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Get data - similar to original code\n",
    "def get_data(file_path=\"Trajectory2.csv\"):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check columns\n",
    "        if 'x_Traject' in df.columns:\n",
    "            data = df['x_Traject'].values.reshape(-1, 1)\n",
    "        else:\n",
    "            # No header? Let's try again\n",
    "            df = pd.read_csv(file_path, header=None)\n",
    "            data = df.values\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't load data: {e}\")\n",
    "        print(\"Using synthetic data instead\")\n",
    "        \n",
    "        # Make fake trajectory data - temperature setpoint scenario\n",
    "        t = np.linspace(0, 1, 7201)\n",
    "        data = np.zeros((len(t), 1))\n",
    "        \n",
    "        # Let's make something interesting with segments\n",
    "        data[:1000] = 45.0  # Starting temperature\n",
    "        \n",
    "        # Ramp up\n",
    "        for i in range(1000, 3000):\n",
    "            progress = (i - 1000) / 2000\n",
    "            data[i] = 45.0 + progress * (70.0 - 45.0)\n",
    "            \n",
    "        # Hold temp\n",
    "        data[3000:5000] = 70.0\n",
    "        \n",
    "        # Ramp down\n",
    "        for i in range(5000, 6000):\n",
    "            progress = (i - 5000) / 1000\n",
    "            data[i] = 70.0 - progress * (70.0 - 50.0)\n",
    "            \n",
    "        # Final hold\n",
    "        data[6000:] = 50.0\n",
    "        \n",
    "        # Add a bit of noise to make it realistic\n",
    "        data += np.random.normal(0, 0.1, size=data.shape)\n",
    "        \n",
    "    return data.astype(np.float32)  # Cast to float32 for faster tensor operations\n",
    "\n",
    "# Modified: Training function for the RL controller\n",
    "def train_rl_controller(env, agent, episodes=100):\n",
    "    \"\"\"Train the controller agent for the specified number of episodes\"\"\"\n",
    "    rewards_history = []\n",
    "    loss_history = []\n",
    "    tracking_errors = []\n",
    "    control_efforts = []\n",
    "    \n",
    "    # Use mini-batches for faster training\n",
    "    for episode in range(episodes):\n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        episode_losses = []\n",
    "        episode_errors = []\n",
    "        episode_efforts = []\n",
    "        done = False\n",
    "        \n",
    "        # Initialize batch tracking\n",
    "        steps = 0\n",
    "        update_frequency = 4  # Update every N steps instead of every step\n",
    "        \n",
    "        while not done:\n",
    "            # Select and take action\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Calculate tracking error and control effort for logging\n",
    "            current_position = env.position_history[-1]\n",
    "            current_setpoint = env.get_current_setpoint()\n",
    "            control_signal = env.action_values[action]\n",
    "            \n",
    "            tracking_error = abs(current_position - current_setpoint)\n",
    "            control_effort = abs(control_signal)\n",
    "            \n",
    "            episode_errors.append(tracking_error)\n",
    "            episode_efforts.append(control_effort)\n",
    "            \n",
    "            # Store in memory\n",
    "            agent.memory.add(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            # Train the network less frequently\n",
    "            if steps % update_frequency == 0:\n",
    "                loss = agent.train()\n",
    "                if loss > 0:\n",
    "                    episode_losses.append(loss)\n",
    "        \n",
    "        # Update target network periodically using soft updates\n",
    "        agent.update_target_network()\n",
    "        \n",
    "        # Decay exploration rate\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Track metrics\n",
    "        rewards_history.append(total_reward)\n",
    "        if episode_losses:\n",
    "            loss_history.append(np.mean(episode_losses))\n",
    "        else:\n",
    "            loss_history.append(0)\n",
    "        tracking_errors.append(np.mean(episode_errors))\n",
    "        control_efforts.append(np.mean(episode_efforts))\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 5 == 0:  # Less frequent logging\n",
    "            print(f\"Episode {episode+1}/{episodes} - Reward: {total_reward:.4f}, \"\n",
    "                  f\"Loss: {np.mean(episode_losses) if episode_losses else 0:.4f}, Epsilon: {agent.epsilon:.4f}, \"\n",
    "                  f\"Mean Error: {np.mean(episode_errors):.4f}, Mean Control: {np.mean(episode_efforts):.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'rewards': rewards_history,\n",
    "        'losses': loss_history,\n",
    "        'tracking_errors': tracking_errors,\n",
    "        'control_efforts': control_efforts\n",
    "    }\n",
    "\n",
    "# Modified: Evaluate the trained controller\n",
    "def evaluate_controller(env, agent, scaler=None):\n",
    "    \"\"\"Evaluate the control performance on the reference trajectory\"\"\"\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # Create arrays to store results\n",
    "    positions = []\n",
    "    setpoints = []\n",
    "    control_signals = []\n",
    "    \n",
    "    # Set evaluation mode\n",
    "    agent.training_mode = False\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        while not done:\n",
    "            # Select best action (no exploration)\n",
    "            action = agent.select_action(state, training=False)\n",
    "            \n",
    "            # Get setpoint before step\n",
    "            current_setpoint = env.get_current_setpoint()\n",
    "            \n",
    "            # Take step\n",
    "            next_state, _, done = env.step(action)\n",
    "            \n",
    "            # Record results\n",
    "            positions.append(env.position_history[-1])  # Last position\n",
    "            setpoints.append(current_setpoint)\n",
    "            control_signals.append(env.action_values[action])\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    positions = np.array(positions)\n",
    "    setpoints = np.array(setpoints)\n",
    "    control_signals = np.array(control_signals)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    tracking_error = np.mean(np.abs(positions - setpoints))\n",
    "    control_effort = np.mean(np.abs(control_signals))\n",
    "    \n",
    "    print(f\"Evaluation Results:\")\n",
    "    print(f\"Mean Tracking Error: {tracking_error:.4f}\")\n",
    "    print(f\"Mean Control Effort: {control_effort:.4f}\")\n",
    "    \n",
    "    # Rescale if scaler is provided\n",
    "    if scaler is not None:\n",
    "        positions = scaler.inverse_transform(positions.reshape(-1, 1)).flatten()\n",
    "        setpoints = scaler.inverse_transform(np.array(setpoints).reshape(-1, 1)).flatten()\n",
    "    \n",
    "    return {\n",
    "        'positions': positions,\n",
    "        'setpoints': setpoints,\n",
    "        'control_signals': control_signals,\n",
    "        'tracking_error': tracking_error,\n",
    "        'control_effort': control_effort\n",
    "    }\n",
    "\n",
    "# Create enhanced visualizations for controller performance\n",
    "def create_controller_visualizations(results, training_metrics=None):\n",
    "    \"\"\"Create visualizations for the controller performance\"\"\"\n",
    "    # Set up the figure with a custom layout\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    grid = gridspec.GridSpec(3, 1, height_ratios=[2, 1, 1])\n",
    "    \n",
    "    # Extract data from results\n",
    "    positions = results['positions']\n",
    "    setpoints = results['setpoints']\n",
    "    control_signals = results['control_signals']\n",
    "    \n",
    "    # First plot - control performance (position vs setpoint)\n",
    "    perf_ax = plt.subplot(grid[0])\n",
    "    time_steps = np.arange(len(positions))\n",
    "    \n",
    "    perf_ax.plot(time_steps, setpoints, 'r--', linewidth=2, label='Setpoint')\n",
    "    perf_ax.plot(time_steps, positions, 'b-', linewidth=1.5, label='System Position')\n",
    "    \n",
    "    perf_ax.set_title('Control System Performance', fontsize=14)\n",
    "    perf_ax.set_xlabel('Time Steps', fontsize=12)\n",
    "    perf_ax.set_ylabel('Position', fontsize=12)\n",
    "    perf_ax.grid(True)\n",
    "    perf_ax.legend()\n",
    "    \n",
    "    # Second plot - control signals\n",
    "    control_ax = plt.subplot(grid[1])\n",
    "    control_ax.plot(time_steps, control_signals, 'g-', linewidth=1.5)\n",
    "    control_ax.set_title('Control Signals', fontsize=14)\n",
    "    control_ax.set_xlabel('Time Steps', fontsize=12)\n",
    "    control_ax.set_ylabel('Control Signal', fontsize=12)\n",
    "    control_ax.grid(True)\n",
    "    \n",
    "    # Third plot - tracking error\n",
    "    error_ax = plt.subplot(grid[2])\n",
    "    tracking_error = np.abs(positions - setpoints)\n",
    "    error_ax.plot(time_steps, tracking_error, 'r-', linewidth=1.5)\n",
    "    error_ax.set_title('Tracking Error', fontsize=14)\n",
    "    error_ax.set_xlabel('Time Steps', fontsize=12)\n",
    "    error_ax.set_ylabel('Error', fontsize=12)\n",
    "    error_ax.grid(True)\n",
    "    \n",
    "    # If training metrics are provided, create additional plots\n",
    "    if training_metrics:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        grid2 = gridspec.GridSpec(2, 2)\n",
    "        \n",
    "        # Plot rewards\n",
    "        rewards_ax = plt.subplot(grid2[0, 0])\n",
    "        rewards_ax.plot(training_metrics['rewards'], 'b-')\n",
    "        rewards_ax.set_title('Training Rewards', fontsize=14)\n",
    "        rewards_ax.set_xlabel('Episode', fontsize=12)\n",
    "        rewards_ax.set_ylabel('Total Reward', fontsize=12)\n",
    "        rewards_ax.grid(True)\n",
    "        \n",
    "        # Plot losses\n",
    "        losses_ax = plt.subplot(grid2[0, 1])\n",
    "        losses_ax.plot(training_metrics['losses'], 'g-')\n",
    "        losses_ax.set_title('Training Loss', fontsize=14)\n",
    "        losses_ax.set_xlabel('Episode', fontsize=12)\n",
    "        losses_ax.set_ylabel('Loss', fontsize=12)\n",
    "        losses_ax.grid(True)\n",
    "        \n",
    "        # Plot tracking errors\n",
    "        errors_ax = plt.subplot(grid2[1, 0])\n",
    "        errors_ax.plot(training_metrics['tracking_errors'], 'r-')\n",
    "        errors_ax.set_title('Training Tracking Errors', fontsize=14)\n",
    "        errors_ax.set_xlabel('Episode', fontsize=12)\n",
    "        errors_ax.set_ylabel('Mean Tracking Error', fontsize=12)\n",
    "        errors_ax.grid(True)\n",
    "        \n",
    "        # Plot control efforts\n",
    "        efforts_ax = plt.subplot(grid2[1, 1])\n",
    "        efforts_ax.plot(training_metrics['control_efforts'], 'orange')\n",
    "        efforts_ax.set_title('Training Control Efforts', fontsize=14)\n",
    "        efforts_ax.set_xlabel('Episode', fontsize=12)\n",
    "        efforts_ax.set_ylabel('Mean Control Effort', fontsize=12)\n",
    "        efforts_ax.grid(True)\n",
    "    \n",
    "    # Make sure everything fits nicely\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save and display\n",
    "    plt.savefig('controller_visualization.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(\"Controller visualization saved as 'controller_visualization.png'\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting RL Controller implementation...\")\n",
    "    \n",
    "    # Configure torch for faster execution\n",
    "    torch.set_num_threads(4)  # Limit number of threads\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True  # Optimize CUDA operations\n",
    "    \n",
    "    # Get reference trajectory data\n",
    "    reference_data = get_data()\n",
    "    print(f\"Got reference trajectory with shape: {reference_data.shape}\")\n",
    "    \n",
    "    # Normalize between 0-1\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(reference_data)\n",
    "    \n",
    "    # Setup the environment\n",
    "    window_size = 50\n",
    "    control_discretization = 20  # Number of discrete control actions\n",
    "    \n",
    "    # Create environment with control-oriented reward\n",
    "    env = RLControlEnvironment(\n",
    "        reference_trajectory=normalized_data,\n",
    "        window_size=window_size,\n",
    "        control_discretization=control_discretization,\n",
    "        use_simulation=True,  # Use simulation mode\n",
    "        control_effort_weight=0.1  # Weight for control effort penalty\n",
    "    )\n",
    "    \n",
    "    # Create the agent\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    agent = FastDQNAgent(\n",
    "        state_size=window_size,\n",
    "        action_size=control_discretization,\n",
    "        hidden_size=64,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Train the controller agent\n",
    "    print(f\"Training RL controller on {device}...\")\n",
    "    training_metrics = train_rl_controller(env, agent, episodes=100)\n",
    "    \n",
    "    # Save the trained controller\n",
    "    torch.save({\n",
    "        'q_network': agent.q_network.state_dict(),\n",
    "        'target_network': agent.target_network.state_dict(),\n",
    "        'rewards': training_metrics['rewards'],\n",
    "        'losses': training_metrics['losses'],\n",
    "        'tracking_errors': training_metrics['tracking_errors'],\n",
    "        'control_efforts': training_metrics['control_efforts'],\n",
    "        'scaler': scaler\n",
    "    }, 'rl_controller_model.pth')\n",
    "    \n",
    "    print(\"Training complete! Evaluating controller and creating visualizations...\")\n",
    "    \n",
    "def main():\n",
    "    print(\"Starting RL Controller implementation...\")\n",
    "    \n",
    "    # Configure torch for faster execution\n",
    "    torch.set_num_threads(4)  # Limit number of threads\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True  # Optimize CUDA operations\n",
    "    \n",
    "    # Get reference trajectory data\n",
    "    reference_data = get_data()\n",
    "    print(f\"Got reference trajectory with shape: {reference_data.shape}\")\n",
    "    \n",
    "    # Normalize between 0-1\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(reference_data)\n",
    "    \n",
    "    # Setup the environment\n",
    "    window_size = 50\n",
    "    control_discretization = 20  # Number of discrete control actions\n",
    "    \n",
    "    # Create environment with control-oriented reward\n",
    "    env = RLControlEnvironment(\n",
    "        reference_trajectory=normalized_data,\n",
    "        window_size=window_size,\n",
    "        control_discretization=control_discretization,\n",
    "        use_simulation=True,  # Use simulation mode\n",
    "        control_effort_weight=0.1  # Weight for control effort penalty\n",
    "    )\n",
    "    \n",
    "    # Create the agent\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    agent = FastDQNAgent(\n",
    "        state_size=window_size,\n",
    "        action_size=control_discretization,\n",
    "        hidden_size=64,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Train the controller agent\n",
    "    print(f\"Training RL controller on {device}...\")\n",
    "    training_metrics = train_rl_controller(env, agent, episodes=100)\n",
    "    \n",
    "    # Save the trained controller\n",
    "    torch.save({\n",
    "        'q_network': agent.q_network.state_dict(),\n",
    "        'target_network': agent.target_network.state_dict(),\n",
    "        'rewards': training_metrics['rewards'],\n",
    "        'losses': training_metrics['losses'],\n",
    "        'tracking_errors': training_metrics['tracking_errors'],\n",
    "        'control_efforts': training_metrics['control_efforts'],\n",
    "        'scaler': scaler\n",
    "    }, 'rl_controller_model.pth')\n",
    "    \n",
    "    print(\"Training complete! Evaluating controller and creating visualizations...\")\n",
    "    \n",
    "    # Evaluate the trained controller\n",
    "    evaluation_results = evaluate_controller(env, agent, scaler)\n",
    "    \n",
    "    # Create visualizations for controller performance\n",
    "    create_controller_visualizations(evaluation_results, training_metrics)\n",
    "    \n",
    "    print(\"RL Controller implementation completed successfully!\")\n",
    "\n",
    "# Run the main function when the script is executed\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1362b9-1674-4ff5-adcb-46309654572b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py310)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
